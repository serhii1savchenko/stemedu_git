\chapter{Вычисления на суперкомпьютере}

Для решения вычислительных задач, требующих большого времени вычислений или больших объемов памяти,  разработаны специальные функции,  которые предоставляют пользователю ресурсы 
суперкомпьютера.  При использовании этих функций вычисления производятся не на одном процессоре,  а на выделенном множестве ядер суперкомпьютера,  количество которых заказывает пользователь. 
Имеются следующие функции,  которые используют суперкомпьютер (парфункции). 

1) \comm{matMultPar1x8}{}~--- вычисление произведения двух матриц;

2) \comm{adjointDetPar}{}~--- вычисление присоединенной матрицы;

3) \comm{charPolPar}{}~--- вычисление характеристического полинома матрицы;

4) \comm{polMultPar}{}~--- вычисление произведения двух полиномов;

5) \comm{BellmanEquationPar}{A}~--- решение однородного уравнения Беллмана $Ax=x$;

6) \comm{BellmanEquationPar}{A,b}~--- решение однородного уравнения Беллмана $Ax+b=x$;

7) \comm{BellmanInequalityPar}{A}~--- решение однородного неравенства Беллмана $Ax\leq x$;

8) \comm{BellmanInequalityPar}{A,b}~--- решение однородного неравенства Беллмана $Ax+b\leq x$; 

 
До применения любой из этих функций пользователь должен указать параметры,  определяющие параллельное окружение: 

$TOTALNODES$~--- общее количество узлов кластера, которые выделяются для вычислений,  

$PROCPERNODE$~--- количество MPI-процессов, запускаемых на одном узле,   

$CLUSTERTIME$~--- максимальное время (в минутах) выполнения программы,  после истечения которого программа принудительно завершится,

$MAXCLUSTERMEMORY$~--- объем памяти, выделяемый для JVM для одного MPI-процесса (опция -Xmx).

Для задания количества ядер на одном узле пользователь должен знать, какой кластер используется и сколько доступно ему ядер на узле.  По умолчанию параметры $TOTALNODES$ и  $PROCPERNODE$ устанавливаются так,  чтобы использовалась половина всех узлов кластера и на каждом узле было запущено по одному процессу,  а $CLUSTERTIME$ двум минутам. Если на одном узле запускается $K$ процессов, то каждому из них будет выделено $MAXCLUSTERMEMORY/K$ мегабайт памяти. 


\section{Параллельные полиномиальные вычисления}
  Для параллельного вычисления произведения полиномов надо использовать команду 
  \comm{polMultPar}{(p1, p2)},  где $p1$,  $p2$~--- входные полиномы.  

\underline{Пример. }
\begin{verbatim}
TOTALNODES = 2;
PROCPERNODE = 1;
A=x^2+3y;
B=x^2+3y+3z;
\polMultPar(A, B);
\end{verbatim}

      
\section{Параллельные матричные вычисления}
  Для параллельного вычисления произведения матриц $m1$ и $m2$ необходимо использовать команду 
  \underline{Пример. }
\begin{verbatim}
TOTALNODES = 2;
PROCPERNODE = 1;
A=[[0,1],[2,3]];
B=[[0,1],[2,3]];
\matMultPar1x8(A, B);
\end{verbatim}
 
  Для параллельного вычисления присоединенной матрицы для матрицы $m$ можно использовать команду 
   \underline{Пример. }
\begin{verbatim}
TOTALNODES = 2;
PROCPERNODE = 1;
SPACE = Z[x];
A=[[0,1],[2,3]];   
\adjointDetPar(A);
\end{verbatim}
 
 
\section{Запуск собственных параллельных программ}
  Mathpar позволяет загружать и исполнять собственные параллельные программы.
  Пакет с программой должен распологаться в корневой директории проекта mathpar.
  Для того, чтобы ваша программа смогла взаимодействовать с системой управления заданиями,
  необходимо в ваш main-метод добавить строку инициализации QueryResult queryRes=Tools.getDataFromClusterRootNode(args)
 (сразу после MPI.Init) и  строку завершения Tools.sendFinishMessage(args) (перед MPI.Finalize),
  этот код будет одинаков для всех ваших программ).   Также вы можете передать вашей программе 
  любые аргументы из web-интерфейса Mathpartner.  Внутри программы их можно получить, вызвав 
метод queryRes.getData(). Ниже приведен пример параллельной программы, 
которая просто выводит в стандартный поток вывода  переданные ей аргументы .

\begin{verbatim}
        MPI.Init(args);
        QueryResult queryRes=Tools.getDataFromClusterRootNode(args);
        int myRank=MPI.COMM_WORLD.getRank();
        if (myRank == 0) {
            Object []ar=queryRes.getData();
            System.out.println("test...");           
            for (int i=0; i<ar.length; i++){
                System.out.println(((Element)ar[i]).intValue());
            }            
        }
        Tools.sendFinishMessage(args);
        MPI.Finalize();
\end{verbatim}


  Далее программу нужно скомпилировать, и папку с программой запаковать в zip-архив.
  Затем нужно загрузить полученный архив на сервер, воспользовавшись вкладкой "файлы" и нажав кнопку "загрузить файл".
  Далее вся работа будет выполняться с помощью функций mathpar. 

Оперативная память делится между всеми ядрами процессора поровну. Для примера, если на узле кластера имеется 8GB памяти,
то если вы запросили 4 ядра на одном процессоре, каждому будет выделено 2GB, а если одно ядро - то оно получит все 8GB.

Команда для загрузки вашего zip-архива, в котором скомпилированные java-классы,
выглядит следующим образом:

 \comm{uploadToCluster}{(FileName)}, где FileName - имя zip-архива.

Чтобы просмотреть список всех ваших загруженных на кластер файлов, используется команда

 \comm{showFileList}{()}.


Для запуска вашей программы используется команда

\comm{runUploadedClass}{(archieveName, classPath, param0, param1,...)},
где archieveName - имя загруженного zip-архива с программой, classPath - путь до класса, содержащего main-метод (с указанием пакетов),
paramX - произвольные параметры, указанные через запятую, которые будут переданы в вашу программу.

Чтобы следить за работой запущенной программы, используется команда 

\comm{getStatus}{(taskID)}

Также имеется возможность получить список всех задач текущего пользователя с описанием их состояний:

\comm{showTaskList}{()}

Для того, чтобы получить содержимое файлов с потоком стандартного вывода/ошибок, используются команды

\comm{getOut}{(taskID)}

\comm{getErr}{(taskID)}

Файлы задачи (файлы, содержащие поток вывода/ошибок) хранятся на кластере двое суток,
zip-архивы, содержащие скомпилированные java-классы, хранятся 30 дней.

